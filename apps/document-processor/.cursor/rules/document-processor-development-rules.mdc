---
description: Document processor-specific development rules for FastAPI with Python
globs: ["apps/document-processor/**/*", "src/**/*", "processors/**/*", "embeddings/**/*", "rag/**/*", "workers/**/*"]
alwaysApply: true
---

# Document Processor Development Rules

## 🎯 **Document Processor-Specific Standards**
These rules are specific to the FastAPI-based document processing service. Follow these in addition to the common development standards.

---

## 🏗️ **FastAPI Architecture Patterns**

### **Modular Service Architecture**
```python
# ✅ CORRECT - Modular FastAPI application structure
from fastapi import FastAPI, Depends
from contextlib import asynccontextmanager

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    await startup_services()
    yield
    # Shutdown
    await shutdown_services()

def create_app() -> FastAPI:
    app = FastAPI(
        title="Document Processor API",
        version="1.0.0",
        lifespan=lifespan
    )
    
    # Register routers
    app.include_router(documents.router, prefix="/api/v1")
    app.include_router(search.router, prefix="/api/v1")
    app.include_router(chat.router, prefix="/api/v1")
    app.include_router(health.router, prefix="/health")
    
    # Add middleware
    app.add_middleware(CorrelationIdMiddleware)
    app.add_middleware(LoggingMiddleware)
    app.add_middleware(ErrorHandlingMiddleware)
    
    return app

app = create_app()
```

### **Dependency Injection Pattern**
```python
# ✅ CORRECT - Dependency injection for services
from typing import Annotated
from fastapi import Depends

class DocumentService:
    def __init__(
        self,
        processor_factory: ProcessorFactory,
        embedding_service: EmbeddingService,
        vector_store: VectorStore,
        logger: Logger
    ):
        self.processor_factory = processor_factory
        self.embedding_service = embedding_service
        self.vector_store = vector_store
        self.logger = logger

    async def process_document(
        self, 
        file_path: Path, 
        user_id: str,
        organization_id: str
    ) -> ProcessingResult:
        """Process document and generate embeddings."""
        try:
            # Get appropriate processor
            processor = self.processor_factory.get_processor(file_path.suffix)
            
            # Extract content
            content = await processor.extract_content(file_path)
            
            # Generate embeddings
            embeddings = await self.embedding_service.generate_embeddings(
                content.chunks
            )
            
            # Store in vector database
            await self.vector_store.store_embeddings(
                embeddings, 
                organization_id,
                content.metadata
            )
            
            return ProcessingResult(
                success=True,
                document_id=content.metadata.document_id,
                chunks_processed=len(content.chunks)
            )
            
        except Exception as e:
            self.logger.error(f"Document processing failed: {e}")
            return ProcessingResult(success=False, error=str(e))

# Dependency providers
async def get_document_service() -> DocumentService:
    return DocumentService(
        processor_factory=get_processor_factory(),
        embedding_service=get_embedding_service(),
        vector_store=get_vector_store(),
        logger=get_logger()
    )

# Route with dependency injection
@router.post("/documents/process")
async def process_document(
    file: UploadFile,
    user_id: str,
    organization_id: str,
    service: Annotated[DocumentService, Depends(get_document_service)]
) -> ProcessingResponse:
    result = await service.process_document(file, user_id, organization_id)
    return ProcessingResponse.from_result(result)
```

---

## 📄 **Document Processing Patterns**

### **Factory Pattern for Processors**
```python
# ✅ CORRECT - Factory pattern for document processors
from abc import ABC, abstractmethod
from typing import Protocol

class DocumentProcessor(Protocol):
    """Protocol for document processors."""
    
    async def extract_content(self, file_path: Path) -> DocumentContent:
        """Extract content from document."""
        ...
    
    def supports_file_type(self, file_extension: str) -> bool:
        """Check if processor supports file type."""
        ...

class PDFProcessor:
    """PDF document processor using PyMuPDF."""
    
    def __init__(self, config: ProcessingConfig):
        self.config = config
        self.logger = get_logger(__name__)
    
    async def extract_content(self, file_path: Path) -> DocumentContent:
        """Extract text and metadata from PDF."""
        try:
            import fitz  # PyMuPDF
            
            doc = fitz.open(file_path)
            text_chunks = []
            metadata = DocumentMetadata(
                filename=file_path.name,
                file_type="pdf",
                page_count=len(doc)
            )
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                text = page.get_text()
                
                if text.strip():
                    chunks = self._chunk_text(text, page_num)
                    text_chunks.extend(chunks)
            
            doc.close()
            
            return DocumentContent(
                chunks=text_chunks,
                metadata=metadata
            )
            
        except Exception as e:
            self.logger.error(f"PDF processing failed: {e}")
            raise ProcessingError(f"Failed to process PDF: {e}")
    
    def supports_file_type(self, file_extension: str) -> bool:
        return file_extension.lower() == ".pdf"
    
    def _chunk_text(self, text: str, page_num: int) -> list[TextChunk]:
        """Split text into chunks with overlap."""
        chunks = []
        chunk_size = self.config.chunk_size
        overlap = self.config.chunk_overlap
        
        words = text.split()
        
        for i in range(0, len(words), chunk_size - overlap):
            chunk_words = words[i:i + chunk_size]
            chunk_text = " ".join(chunk_words)
            
            chunks.append(TextChunk(
                text=chunk_text,
                page_number=page_num,
                chunk_index=len(chunks),
                word_count=len(chunk_words)
            ))
        
        return chunks

class ProcessorFactory:
    """Factory for creating document processors."""
    
    def __init__(self, config: ProcessingConfig):
        self.config = config
        self._processors: dict[str, type[DocumentProcessor]] = {
            ".pdf": PDFProcessor,
            ".docx": DOCXProcessor,
            ".pptx": PPTXProcessor,
            ".txt": TextProcessor
        }
    
    def get_processor(self, file_extension: str) -> DocumentProcessor:
        """Get processor for file type."""
        processor_class = self._processors.get(file_extension.lower())
        
        if not processor_class:
            raise UnsupportedFileTypeError(f"No processor for {file_extension}")
        
        return processor_class(self.config)
    
    def register_processor(
        self, 
        file_extension: str, 
        processor_class: type[DocumentProcessor]
    ) -> None:
        """Register new processor for file type."""
        self._processors[file_extension.lower()] = processor_class
```

### **Async Processing with Background Jobs**
```python
# ✅ CORRECT - Background job processing with RQ
from rq import Queue
from rq.job import Job
import redis

class DocumentProcessingWorker:
    """Background worker for document processing."""
    
    def __init__(self, redis_client: redis.Redis):
        self.queue = Queue('document_processing', connection=redis_client)
        self.logger = get_logger(__name__)
    
    def enqueue_processing(
        self,
        file_path: str,
        user_id: str,
        organization_id: str,
        document_id: str
    ) -> str:
        """Enqueue document processing job."""
        job = self.queue.enqueue(
            process_document_job,
            file_path=file_path,
            user_id=user_id,
            organization_id=organization_id,
            document_id=document_id,
            job_timeout='10m',
            result_ttl=3600  # Keep result for 1 hour
        )
        
        self.logger.info(f"Enqueued processing job {job.id} for document {document_id}")
        return job.id
    
    def get_job_status(self, job_id: str) -> JobStatus:
        """Get status of processing job."""
        try:
            job = Job.fetch(job_id, connection=self.queue.connection)
            
            return JobStatus(
                id=job.id,
                status=job.get_status(),
                progress=job.meta.get('progress', 0),
                result=job.result,
                error=job.exc_info
            )
        except Exception as e:
            self.logger.error(f"Failed to get job status: {e}")
            return JobStatus(id=job_id, status='failed', error=str(e))

# Background job function
async def process_document_job(
    file_path: str,
    user_id: str,
    organization_id: str,
    document_id: str
) -> ProcessingResult:
    """Background job for document processing."""
    logger = get_logger(__name__)
    
    try:
        # Update job progress
        job = get_current_job()
        job.meta['progress'] = 10
        job.save_meta()
        
        # Initialize services
        document_service = get_document_service()
        
        # Process document
        logger.info(f"Starting processing for document {document_id}")
        
        result = await document_service.process_document(
            Path(file_path),
            user_id,
            organization_id
        )
        
        job.meta['progress'] = 100
        job.save_meta()
        
        logger.info(f"Completed processing for document {document_id}")
        return result
        
    except Exception as e:
        logger.error(f"Document processing job failed: {e}")
        raise ProcessingError(f"Processing failed: {e}")
```

---

## 🧠 **Embedding & Vector Operations**

### **Embedding Provider Pattern**
```python
# ✅ CORRECT - Abstract embedding provider with multiple implementations
from abc import ABC, abstractmethod
from typing import List, Optional

class EmbeddingProvider(ABC):
    """Abstract base class for embedding providers."""
    
    @abstractmethod
    async def generate_embeddings(
        self, 
        texts: List[str]
    ) -> List[List[float]]:
        """Generate embeddings for list of texts."""
        pass
    
    @abstractmethod
    def get_embedding_dimension(self) -> int:
        """Get dimension of embeddings."""
        pass
    
    @abstractmethod
    def get_model_name(self) -> str:
        """Get name of embedding model."""
        pass

class OllamaEmbeddingProvider(EmbeddingProvider):
    """Ollama embedding provider."""
    
    def __init__(self, config: EmbeddingConfig):
        self.config = config
        self.model_name = config.ollama_model
        self.base_url = config.ollama_base_url
        self.logger = get_logger(__name__)
    
    async def generate_embeddings(
        self, 
        texts: List[str]
    ) -> List[List[float]]:
        """Generate embeddings using Ollama."""
        try:
            import httpx
            
            embeddings = []
            
            async with httpx.AsyncClient() as client:
                for text in texts:
                    response = await client.post(
                        f"{self.base_url}/api/embeddings",
                        json={
                            "model": self.model_name,
                            "prompt": text
                        },
                        timeout=30.0
                    )
                    
                    response.raise_for_status()
                    result = response.json()
                    embeddings.append(result["embedding"])
            
            self.logger.info(f"Generated {len(embeddings)} embeddings")
            return embeddings
            
        except Exception as e:
            self.logger.error(f"Ollama embedding generation failed: {e}")
            raise EmbeddingError(f"Failed to generate embeddings: {e}")
    
    def get_embedding_dimension(self) -> int:
        # mxbai-embed-large has 1024 dimensions
        return 1024
    
    def get_model_name(self) -> str:
        return self.model_name

class OpenAIEmbeddingProvider(EmbeddingProvider):
    """OpenAI embedding provider."""
    
    def __init__(self, config: EmbeddingConfig):
        self.config = config
        self.client = openai.AsyncOpenAI(api_key=config.openai_api_key)
        self.model_name = config.openai_model
        self.logger = get_logger(__name__)
    
    async def generate_embeddings(
        self, 
        texts: List[str]
    ) -> List[List[float]]:
        """Generate embeddings using OpenAI."""
        try:
            # Batch process texts for efficiency
            response = await self.client.embeddings.create(
                model=self.model_name,
                input=texts
            )
            
            embeddings = [item.embedding for item in response.data]
            
            self.logger.info(f"Generated {len(embeddings)} embeddings")
            return embeddings
            
        except Exception as e:
            self.logger.error(f"OpenAI embedding generation failed: {e}")
            raise EmbeddingError(f"Failed to generate embeddings: {e}")
    
    def get_embedding_dimension(self) -> int:
        # text-embedding-3-small has 1536 dimensions
        return 1536
    
    def get_model_name(self) -> str:
        return self.model_name

class EmbeddingService:
    """Service for managing embeddings."""
    
    def __init__(self, provider: EmbeddingProvider):
        self.provider = provider
        self.logger = get_logger(__name__)
    
    async def generate_embeddings(
        self, 
        chunks: List[TextChunk]
    ) -> List[EmbeddingChunk]:
        """Generate embeddings for text chunks."""
        texts = [chunk.text for chunk in chunks]
        
        try:
            embeddings = await self.provider.generate_embeddings(texts)
            
            embedding_chunks = []
            for chunk, embedding in zip(chunks, embeddings):
                embedding_chunks.append(EmbeddingChunk(
                    text=chunk.text,
                    embedding=embedding,
                    metadata=chunk.metadata,
                    model_name=self.provider.get_model_name()
                ))
            
            return embedding_chunks
            
        except Exception as e:
            self.logger.error(f"Embedding generation failed: {e}")
            raise
```

### **Vector Database Operations**
```python
# ✅ CORRECT - QDrant vector database operations
from qdrant_client import QdrantClient
from qdrant_client.models import (
    Distance, VectorParams, PointStruct, Filter, 
    FieldCondition, MatchValue, SearchRequest
)

class QDrantVectorStore:
    """QDrant vector database operations."""
    
    def __init__(self, config: VectorStoreConfig):
        self.client = QdrantClient(
            host=config.qdrant_host,
            port=config.qdrant_port,
            api_key=config.qdrant_api_key
        )
        self.config = config
        self.logger = get_logger(__name__)
    
    async def ensure_collection(
        self, 
        collection_name: str, 
        vector_size: int
    ) -> None:
        """Ensure collection exists with proper configuration."""
        try:
            collections = await self.client.get_collections()
            collection_names = [c.name for c in collections.collections]
            
            if collection_name not in collection_names:
                await self.client.create_collection(
                    collection_name=collection_name,
                    vectors_config=VectorParams(
                        size=vector_size,
                        distance=Distance.COSINE
                    )
                )
                self.logger.info(f"Created collection: {collection_name}")
            
        except Exception as e:
            self.logger.error(f"Failed to ensure collection: {e}")
            raise VectorStoreError(f"Collection setup failed: {e}")
    
    async def store_embeddings(
        self,
        embeddings: List[EmbeddingChunk],
        organization_id: str,
        document_id: str
    ) -> None:
        """Store embeddings in vector database."""
        try:
            collection_name = f"org_{organization_id}"
            
            # Ensure collection exists
            if embeddings:
                vector_size = len(embeddings[0].embedding)
                await self.ensure_collection(collection_name, vector_size)
            
            # Prepare points for insertion
            points = []
            for i, embedding in enumerate(embeddings):
                point = PointStruct(
                    id=f"{document_id}_{i}",
                    vector=embedding.embedding,
                    payload={
                        "document_id": document_id,
                        "organization_id": organization_id,
                        "text": embedding.text,
                        "chunk_index": i,
                        "model_name": embedding.model_name,
                        "created_at": datetime.utcnow().isoformat(),
                        **embedding.metadata
                    }
                )
                points.append(point)
            
            # Insert points in batches
            batch_size = self.config.batch_size
            for i in range(0, len(points), batch_size):
                batch = points[i:i + batch_size]
                await self.client.upsert(
                    collection_name=collection_name,
                    points=batch
                )
            
            self.logger.info(
                f"Stored {len(embeddings)} embeddings for document {document_id}"
            )
            
        except Exception as e:
            self.logger.error(f"Failed to store embeddings: {e}")
            raise VectorStoreError(f"Storage failed: {e}")
    
    async def search_similar(
        self,
        query_embedding: List[float],
        organization_id: str,
        limit: int = 10,
        score_threshold: float = 0.7,
        document_ids: Optional[List[str]] = None
    ) -> List[SearchResult]:
        """Search for similar embeddings."""
        try:
            collection_name = f"org_{organization_id}"
            
            # Build filter
            filter_conditions = [
                FieldCondition(
                    key="organization_id",
                    match=MatchValue(value=organization_id)
                )
            ]
            
            if document_ids:
                filter_conditions.append(
                    FieldCondition(
                        key="document_id",
                        match=MatchValue(value=document_ids)
                    )
                )
            
            # Perform search
            search_result = await self.client.search(
                collection_name=collection_name,
                query_vector=query_embedding,
                query_filter=Filter(must=filter_conditions),
                limit=limit,
                score_threshold=score_threshold
            )
            
            # Convert to search results
            results = []
            for hit in search_result:
                results.append(SearchResult(
                    id=hit.id,
                    score=hit.score,
                    text=hit.payload["text"],
                    document_id=hit.payload["document_id"],
                    chunk_index=hit.payload["chunk_index"],
                    metadata=hit.payload
                ))
            
            self.logger.info(f"Found {len(results)} similar chunks")
            return results
            
        except Exception as e:
            self.logger.error(f"Vector search failed: {e}")
            raise VectorStoreError(f"Search failed: {e}")
```

---

## 🔍 **RAG Implementation Patterns**

### **Retrieval-Augmented Generation Service**
```python
# ✅ CORRECT - RAG service with context assembly
class RAGService:
    """Retrieval-Augmented Generation service."""
    
    def __init__(
        self,
        embedding_service: EmbeddingService,
        vector_store: VectorStore,
        llm_service: LLMService,
        config: RAGConfig
    ):
        self.embedding_service = embedding_service
        self.vector_store = vector_store
        self.llm_service = llm_service
        self.config = config
        self.logger = get_logger(__name__)
    
    async def query(
        self,
        question: str,
        organization_id: str,
        user_id: str,
        document_ids: Optional[List[str]] = None,
        conversation_history: Optional[List[ChatMessage]] = None
    ) -> RAGResponse:
        """Process RAG query and generate response."""
        try:
            # Generate query embedding
            query_embedding = await self.embedding_service.generate_embeddings([question])
            
            # Search for relevant chunks
            search_results = await self.vector_store.search_similar(
                query_embedding=query_embedding[0].embedding,
                organization_id=organization_id,
                limit=self.config.max_chunks,
                score_threshold=self.config.similarity_threshold,
                document_ids=document_ids
            )
            
            if not search_results:
                return RAGResponse(
                    answer="I couldn't find relevant information to answer your question.",
                    confidence=0.0,
                    sources=[]
                )
            
            # Assemble context
            context = self._assemble_context(search_results, conversation_history)
            
            # Generate response
            response = await self.llm_service.generate_response(
                question=question,
                context=context,
                conversation_history=conversation_history
            )
            
            # Calculate confidence score
            confidence = self._calculate_confidence(search_results, response)
            
            # Extract citations
            citations = self._extract_citations(search_results)
            
            return RAGResponse(
                answer=response.content,
                confidence=confidence,
                sources=citations,
                search_results=search_results
            )
            
        except Exception as e:
            self.logger.error(f"RAG query failed: {e}")
            raise RAGError(f"Query processing failed: {e}")
    
    def _assemble_context(
        self,
        search_results: List[SearchResult],
        conversation_history: Optional[List[ChatMessage]] = None
    ) -> str:
        """Assemble context from search results and conversation history."""
        context_parts = []
        
        # Add conversation history if provided
        if conversation_history:
            history_context = self._format_conversation_history(conversation_history)
            context_parts.append(f"Conversation History:\n{history_context}\n")
        
        # Add retrieved chunks
        context_parts.append("Relevant Information:")
        
        for i, result in enumerate(search_results, 1):
            context_parts.append(
                f"\n[Source {i}] (Score: {result.score:.3f})\n{result.text}"
            )
        
        return "\n".join(context_parts)
    
    def _calculate_confidence(
        self,
        search_results: List[SearchResult],
        response: LLMResponse
    ) -> float:
        """Calculate confidence score for the response."""
        if not search_results:
            return 0.0
        
        # Base confidence on search result scores
        avg_score = sum(r.score for r in search_results) / len(search_results)
        
        # Adjust based on number of sources
        source_factor = min(len(search_results) / self.config.optimal_chunk_count, 1.0)
        
        # Adjust based on response quality indicators
        response_factor = 1.0
        if hasattr(response, 'confidence'):
            response_factor = response.confidence
        
        confidence = avg_score * source_factor * response_factor
        return min(confidence, 1.0)
    
    def _extract_citations(self, search_results: List[SearchResult]) -> List[Citation]:
        """Extract citations from search results."""
        citations = []
        
        for result in search_results:
            citation = Citation(
                document_id=result.document_id,
                chunk_index=result.chunk_index,
                text_preview=result.text[:200] + "..." if len(result.text) > 200 else result.text,
                score=result.score,
                page_number=result.metadata.get("page_number"),
                filename=result.metadata.get("filename")
            )
            citations.append(citation)
        
        return citations

class LLMService:
    """Service for LLM interactions."""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.logger = get_logger(__name__)
        
        # Initialize provider based on config
        if config.provider == "openai":
            self.client = openai.AsyncOpenAI(api_key=config.api_key)
        elif config.provider == "anthropic":
            self.client = anthropic.AsyncAnthropic(api_key=config.api_key)
        else:
            raise ValueError(f"Unsupported LLM provider: {config.provider}")
    
    async def generate_response(
        self,
        question: str,
        context: str,
        conversation_history: Optional[List[ChatMessage]] = None
    ) -> LLMResponse:
        """Generate response using LLM."""
        try:
            # Build prompt
            prompt = self._build_prompt(question, context, conversation_history)
            
            if self.config.provider == "openai":
                response = await self._generate_openai_response(prompt)
            elif self.config.provider == "anthropic":
                response = await self._generate_anthropic_response(prompt)
            
            return response
            
        except Exception as e:
            self.logger.error(f"LLM response generation failed: {e}")
            raise LLMError(f"Response generation failed: {e}")
    
    def _build_prompt(
        self,
        question: str,
        context: str,
        conversation_history: Optional[List[ChatMessage]] = None
    ) -> str:
        """Build prompt for LLM."""
        prompt_parts = [
            "You are a helpful AI assistant that answers questions based on provided context.",
            "Use the following information to answer the user's question.",
            "If the context doesn't contain enough information to answer the question, say so clearly.",
            "Always cite your sources when possible.",
            "",
            f"Context:\n{context}",
            "",
            f"Question: {question}",
            "",
            "Answer:"
        ]
        
        return "\n".join(prompt_parts)
    
    async def _generate_openai_response(self, prompt: str) -> LLMResponse:
        """Generate response using OpenAI."""
        response = await self.client.chat.completions.create(
            model=self.config.model_name,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=self.config.max_tokens,
            temperature=self.config.temperature
        )
        
        return LLMResponse(
            content=response.choices[0].message.content,
            model=self.config.model_name,
            tokens_used=response.usage.total_tokens
        )
```

---

## 🧪 **Testing Patterns**

### **Unit Testing with pytest**
```python
# ✅ CORRECT - Comprehensive unit tests
import pytest
from unittest.mock import Mock, AsyncMock, patch
from pathlib import Path

class TestDocumentService:
    """Test suite for DocumentService."""
    
    @pytest.fixture
    def mock_dependencies(self):
        """Create mock dependencies."""
        return {
            'processor_factory': Mock(),
            'embedding_service': AsyncMock(),
            'vector_store': AsyncMock(),
            'logger': Mock()
        }
    
    @pytest.fixture
    def document_service(self, mock_dependencies):
        """Create DocumentService with mocked dependencies."""
        return DocumentService(**mock_dependencies)
    
    @pytest.fixture
    def sample_document_content(self):
        """Sample document content for testing."""
        return DocumentContent(
            chunks=[
                TextChunk(text="Sample text chunk 1", page_number=1, chunk_index=0),
                TextChunk(text="Sample text chunk 2", page_number=1, chunk_index=1)
            ],
            metadata=DocumentMetadata(
                filename="test.pdf",
                file_type="pdf",
                page_count=1
            )
        )
    
    @pytest.mark.asyncio
    async def test_process_document_success(
        self, 
        document_service, 
        mock_dependencies,
        sample_document_content
    ):
        """Test successful document processing."""
        # Arrange
        file_path = Path("test.pdf")
        user_id = "user-123"
        organization_id = "org-456"
        
        mock_processor = AsyncMock()
        mock_processor.extract_content.return_value = sample_document_content
        mock_dependencies['processor_factory'].get_processor.return_value = mock_processor
        
        mock_embeddings = [
            EmbeddingChunk(text="chunk 1", embedding=[0.1, 0.2, 0.3]),
            EmbeddingChunk(text="chunk 2", embedding=[0.4, 0.5, 0.6])
        ]
        mock_dependencies['embedding_service'].generate_embeddings.return_value = mock_embeddings
        
        # Act
        result = await document_service.process_document(file_path, user_id, organization_id)
        
        # Assert
        assert result.success is True
        assert result.chunks_processed == 2
        
        mock_dependencies['processor_factory'].get_processor.assert_called_once_with(".pdf")
        mock_processor.extract_content.assert_called_once_with(file_path)
        mock_dependencies['embedding_service'].generate_embeddings.assert_called_once()
        mock_dependencies['vector_store'].store_embeddings.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_process_document_unsupported_file_type(
        self, 
        document_service, 
        mock_dependencies
    ):
        """Test processing unsupported file type."""
        # Arrange
        file_path = Path("test.xyz")
        user_id = "user-123"
        organization_id = "org-456"
        
        mock_dependencies['processor_factory'].get_processor.side_effect = UnsupportedFileTypeError("Unsupported file type")
        
        # Act
        result = await document_service.process_document(file_path, user_id, organization_id)
        
        # Assert
        assert result.success is False
        assert "Unsupported file type" in result.error
    
    @pytest.mark.asyncio
    async def test_process_document_embedding_failure(
        self, 
        document_service, 
        mock_dependencies,
        sample_document_content
    ):
        """Test handling of embedding generation failure."""
        # Arrange
        file_path = Path("test.pdf")
        user_id = "user-123"
        organization_id = "org-456"
        
        mock_processor = AsyncMock()
        mock_processor.extract_content.return_value = sample_document_content
        mock_dependencies['processor_factory'].get_processor.return_value = mock_processor
        
        mock_dependencies['embedding_service'].generate_embeddings.side_effect = EmbeddingError("Embedding failed")
        
        # Act
        result = await document_service.process_document(file_path, user_id, organization_id)
        
        # Assert
        assert result.success is False
        assert "Embedding failed" in result.error

class TestPDFProcessor:
    """Test suite for PDFProcessor."""
    
    @pytest.fixture
    def pdf_processor(self):
        """Create PDFProcessor instance."""
        config = ProcessingConfig(chunk_size=100, chunk_overlap=20)
        return PDFProcessor(config)
    
    @pytest.mark.asyncio
    async def test_extract_content_success(self, pdf_processor, tmp_path):
        """Test successful PDF content extraction."""
        # Create a test PDF file
        pdf_path = tmp_path / "test.pdf"
        
        with patch('fitz.open') as mock_fitz:
            # Mock PyMuPDF document
            mock_doc = Mock()
            mock_page = Mock()
            mock_page.get_text.return_value = "This is sample PDF text content."
            mock_doc.__len__.return_value = 1
            mock_doc.__getitem__.return_value = mock_page
            mock_fitz.return_value = mock_doc
            
            # Act
            result = await pdf_processor.extract_content(pdf_path)
            
            # Assert
            assert isinstance(result, DocumentContent)
            assert len(result.chunks) > 0
            assert result.metadata.file_type == "pdf"
            assert result.metadata.page_count == 1
            
            mock_fitz.assert_called_once_with(pdf_path)
            mock_doc.close.assert_called_once()
    
    def test_supports_file_type(self, pdf_processor):
        """Test file type support check."""
        assert pdf_processor.supports_file_type(".pdf") is True
        assert pdf_processor.supports_file_type(".PDF") is True
        assert pdf_processor.supports_file_type(".docx") is False
    
    def test_chunk_text(self, pdf_processor):
        """Test text chunking functionality."""
        text = "This is a sample text that should be chunked into smaller pieces for processing."
        page_num = 1
        
        chunks = pdf_processor._chunk_text(text, page_num)
        
        assert len(chunks) > 0
        assert all(isinstance(chunk, TextChunk) for chunk in chunks)
        assert all(chunk.page_number == page_num for chunk in chunks)
```

### **Integration Testing**
```python
# ✅ CORRECT - Integration tests for API endpoints
import pytest
from httpx import AsyncClient
from fastapi.testclient import TestClient
from pathlib import Path

@pytest.mark.asyncio
class TestDocumentProcessingAPI:
    """Integration tests for document processing API."""
    
    @pytest.fixture
    async def client(self):
        """Create test client."""
        from main import app
        async with AsyncClient(app=app, base_url="http://test") as client:
            yield client
    
    @pytest.fixture
    def sample_pdf_file(self, tmp_path):
        """Create sample PDF file for testing."""
        pdf_path = tmp_path / "sample.pdf"
        # Create a minimal PDF file for testing
        pdf_path.write_bytes(b"%PDF-1.4\n1 0 obj\n<<\n/Type /Catalog\n/Pages 2 0 R\n>>\nendobj\n")
        return pdf_path
    
    async def test_upload_and_process_document(self, client, sample_pdf_file):
        """Test document upload and processing workflow."""
        # Upload document
        with open(sample_pdf_file, "rb") as f:
            response = await client.post(
                "/api/v1/documents/upload",
                files={"file": ("sample.pdf", f, "application/pdf")},
                data={
                    "user_id": "test-user",
                    "organization_id": "test-org"
                }
            )
        
        assert response.status_code == 200
        upload_data = response.json()
        assert upload_data["success"] is True
        
        document_id = upload_data["data"]["document_id"]
        job_id = upload_data["data"]["job_id"]
        
        # Check processing status
        response = await client.get(f"/api/v1/documents/{document_id}/status")
        assert response.status_code == 200
        
        status_data = response.json()
        assert status_data["data"]["document_id"] == document_id
    
    async def test_search_documents(self, client):
        """Test document search functionality."""
        response = await client.post(
            "/api/v1/search",
            json={
                "query": "sample query",
                "organization_id": "test-org",
                "limit": 10
            }
        )
        
        assert response.status_code == 200
        search_data = response.json()
        assert "results" in search_data["data"]
    
    async def test_rag_query(self, client):
        """Test RAG query functionality."""
        response = await client.post(
            "/api/v1/chat/query",
            json={
                "question": "What is the main topic?",
                "organization_id": "test-org",
                "user_id": "test-user"
            }
        )
        
        assert response.status_code == 200
        rag_data = response.json()
        assert "answer" in rag_data["data"]
        assert "confidence" in rag_data["data"]
        assert "sources" in rag_data["data"]
```

---

## 📊 **Performance & Monitoring**

### **Performance Optimization**
```python
# ✅ CORRECT - Performance monitoring and optimization
import time
import asyncio
from functools import wraps
from typing import Callable, Any

def monitor_performance(func_name: str):
    """Decorator to monitor function performance."""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def async_wrapper(*args, **kwargs) -> Any:
            start_time = time.time()
            logger = get_logger(__name__)
            
            try:
                result = await func(*args, **kwargs)
                duration = time.time() - start_time
                
                logger.info(f"{func_name} completed", extra={
                    "duration_ms": duration * 1000,
                    "function": func_name,
                    "success": True
                })
                
                return result
                
            except Exception as e:
                duration = time.time() - start_time
                
                logger.error(f"{func_name} failed", extra={
                    "duration_ms": duration * 1000,
                    "function": func_name,
                    "success": False,
                    "error": str(e)
                })
                
                raise
        
        @wraps(func)
        def sync_wrapper(*args, **kwargs) -> Any:
            start_time = time.time()
            logger = get_logger(__name__)
            
            try:
                result = func(*args, **kwargs)
                duration = time.time() - start_time
                
                logger.info(f"{func_name} completed", extra={
                    "duration_ms": duration * 1000,
                    "function": func_name,
                    "success": True
                })
                
                return result
                
            except Exception as e:
                duration = time.time() - start_time
                
                logger.error(f"{func_name} failed", extra={
                    "duration_ms": duration * 1000,
                    "function": func_name,
                    "success": False,
                    "error": str(e)
                })
                
                raise
        
        return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper
    
    return decorator

# Usage
class OptimizedDocumentService:
    """Document service with performance monitoring."""
    
    @monitor_performance("document_processing")
    async def process_document(self, file_path: Path, user_id: str, organization_id: str) -> ProcessingResult:
        """Process document with performance monitoring."""
        # Implementation with monitoring
        pass
    
    @monitor_performance("batch_embedding_generation")
    async def generate_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings in optimized batches."""
        batch_size = 50  # Optimal batch size for most providers
        embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            batch_embeddings = await self.embedding_service.generate_embeddings(batch)
            embeddings.extend(batch_embeddings)
        
        return embeddings
```

### **Health Monitoring**
```python
# ✅ CORRECT - Comprehensive health checks
from fastapi import APIRouter, status
from typing import Dict, Any

router = APIRouter()

class HealthChecker:
    """Health check service."""
    
    def __init__(
        self,
        vector_store: VectorStore,
        embedding_service: EmbeddingService,
        redis_client: redis.Redis
    ):
        self.vector_store = vector_store
        self.embedding_service = embedding_service
        self.redis_client = redis_client
        self.logger = get_logger(__name__)
    
    async def check_vector_store(self) -> Dict[str, Any]:
        """Check vector store health."""
        try:
            # Test connection and basic operation
            collections = await self.vector_store.client.get_collections()
            
            return {
                "status": "healthy",
                "collections_count": len(collections.collections),
                "response_time_ms": 0  # Would measure actual response time
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e)
            }
    
    async def check_embedding_service(self) -> Dict[str, Any]:
        """Check embedding service health."""
        try:
            # Test with a simple embedding generation
            test_embeddings = await self.embedding_service.generate_embeddings(["health check"])
            
            return {
                "status": "healthy",
                "model": self.embedding_service.provider.get_model_name(),
                "dimension": len(test_embeddings[0].embedding)
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e)
            }
    
    async def check_redis(self) -> Dict[str, Any]:
        """Check Redis health."""
        try:
            await self.redis_client.ping()
            info = await self.redis_client.info()
            
            return {
                "status": "healthy",
                "memory_usage": info.get("used_memory_human"),
                "connected_clients": info.get("connected_clients")
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e)
            }
    
    async def get_system_health(self) -> Dict[str, Any]:
        """Get overall system health."""
        checks = await asyncio.gather(
            self.check_vector_store(),
            self.check_embedding_service(),
            self.check_redis(),
            return_exceptions=True
        )
        
        vector_store_health, embedding_health, redis_health = checks
        
        all_healthy = all(
            check.get("status") == "healthy" 
            for check in [vector_store_health, embedding_health, redis_health]
            if isinstance(check, dict)
        )
        
        return {
            "status": "healthy" if all_healthy else "unhealthy",
            "timestamp": datetime.utcnow().isoformat(),
            "checks": {
                "vector_store": vector_store_health,
                "embedding_service": embedding_health,
                "redis": redis_health
            }
        }

@router.get("/health", status_code=status.HTTP_200_OK)
async def health_check(
    health_checker: HealthChecker = Depends(get_health_checker)
) -> Dict[str, Any]:
    """Health check endpoint."""
    health_status = await health_checker.get_system_health()
    
    if health_status["status"] != "healthy":
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=health_status
        )
    
    return health_status
```

---

## 🚫 **Anti-Patterns to Avoid**

### **Document Processing Anti-Patterns**
```python
# ❌ WRONG - Synchronous processing blocking the event loop
def process_document_sync(file_path: str) -> ProcessingResult:
    # This blocks the event loop
    with open(file_path, 'rb') as f:
        content = f.read()
    
    # Heavy processing without yielding control
    chunks = extract_text_chunks(content)
    embeddings = generate_embeddings(chunks)  # Blocking call
    
    return ProcessingResult(chunks=chunks, embeddings=embeddings)

# ✅ CORRECT - Async processing with proper yielding
async def process_document_async(file_path: Path) -> ProcessingResult:
    async with aiofiles.open(file_path, 'rb') as f:
        content = await f.read()
    
    # Process in chunks, yielding control
    chunks = await extract_text_chunks_async(content)
    embeddings = await generate_embeddings_async(chunks)
    
    return ProcessingResult(chunks=chunks, embeddings=embeddings)
```

```python
# ❌ WRONG - Loading entire document into memory
def process_large_pdf(file_path: str) -> List[str]:
    doc = fitz.open(file_path)
    all_text = ""
    
    # Loads entire document into memory
    for page in doc:
        all_text += page.get_text()
    
    return [all_text]  # Single massive chunk

# ✅ CORRECT - Streaming processing
async def process_large_pdf_streaming(file_path: Path) -> AsyncGenerator[str, None]:
    doc = fitz.open(file_path)
    
    try:
        for page_num in range(len(doc)):
            page = doc[page_num]
            text = page.get_text()
            
            # Process page by page, yielding chunks
            chunks = chunk_text(text, page_num)
            for chunk in chunks:
                yield chunk
                
            # Yield control periodically
            if page_num % 10 == 0:
                await asyncio.sleep(0)
    finally:
        doc.close()
```

---

## 📋 **Development Checklist**

### **Before Starting Development**
- [ ] Processing pipeline design documented
- [ ] Embedding strategy selected
- [ ] Vector database schema planned
- [ ] Error handling strategy defined
- [ ] Performance requirements identified

### **During Development**
- [ ] Follow async/await patterns
- [ ] Implement proper error handling
- [ ] Add performance monitoring
- [ ] Write comprehensive tests
- [ ] Document all public APIs
- [ ] Optimize memory usage

### **Before Code Review**
- [ ] All tests passing (unit + integration)
- [ ] Performance benchmarks met
- [ ] Memory usage optimized
- [ ] Error scenarios tested
- [ ] API documentation updated
- [ ] Logging properly implemented

### **Before Deployment**
- [ ] Load testing completed
- [ ] Health checks implemented
- [ ] Monitoring configured
- [ ] Background job processing tested
- [ ] Rollback procedures documented

---

**Remember**: The document processor is the intelligence core of our system. Prioritize **accuracy**, **performance**, and **reliability** in every decision.

*Last Updated: January 2025*